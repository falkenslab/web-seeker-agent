import asyncio
import operator
from typing import TypedDict, Annotated, Iterator

from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver

from .utils import bold, PURPLE

class AgentState(TypedDict):
    """Estado del agente usado por el grafo.

    Contiene la lista de mensajes intercambiados durante la conversaci칩n.

    - messages: lista acumulativa de mensajes; se combina usando ``operator.add``.
    """

    messages: Annotated[list[AnyMessage], operator.add]  # Crea una lista de mensajes que se puede concatenar con el operador +


class Agent:
    """Agente orquestador que combina un modelo LLM con herramientas.

    Construye un grafo de estados con dos nodos principales:
    - "llm": invoca el modelo para generar la siguiente acci칩n o respuesta.
    - "action": ejecuta herramientas solicitadas por el modelo y retorna sus resultados.

    El grafo itera hasta que el modelo no solicite m치s herramientas.

    Atributos:
        system: texto del mensaje de sistema que se a침adir치 una vez al principio.
        graph: grafo compilado de LangGraph que gestiona el flujo de mensajes.
        tools: diccionario de herramientas disponibles indexadas por nombre.
        model: modelo LLM con herramientas enlazadas mediante ``bind_tools``.
    """

    _current_state: AgentState = None

    def __init__(self, model, tools, system: str = "", verbose: bool = False):
        """Inicializa el agente con un modelo, herramientas y un prompt de sistema.

        Par치metros:
            model: instancia del modelo conversacional (por ejemplo, ``ChatOpenAI``).
            tools: lista de herramientas compatibles con LangChain a exponer al modelo.
            system: mensaje de sistema opcional que se antepone solo una vez.
        """
        self.system = system
        self.verbose = verbose
        self.config = {
            "recursion_limit": 50,                              # L칤mite de recursi칩n para evitar bucles infinitos en el grafo (para que no est칠 infinitamente dando vueltas)
            "configurable": { "thread_id" : "1"}                # Identificador del hilo de conversaci칩n (en este caso, 1 agente s칩lo puede mantener una conversaci칩n a la vez)
        }

        graph = StateGraph(AgentState)

        # Definici칩n de nodos del grafo
        graph.add_node("init", self.init_state)                 # Nodo de estado inicial
        graph.add_node("llm", self.call_openai)                 # Nodo de llamada al modelo
        graph.add_node("action", self.take_action)              # Nodo de ejecuci칩n de herramientas (acciones)

        # Definici칩n de aristas del grafo
        graph.add_edge("init", "llm")                           # Desde el estado inicial, ir al modelo        
        graph.add_conditional_edges(
            "llm",                                              # La arista condicional sale del nodo "llm"
            self.exists_action,                                 # Funci칩n que decide si se debe ir al nodo de acci칩n o terminar
            {True: "action", False: END},
        )                                                       # Si el modelo decide llamar a una herramienta, ir al nodo de acci칩n; si no, terminar
        graph.add_edge("action", "llm")                         # Despu칠s de ejecutar una acci칩n, volver al modelo

        # Definici칩n del punto de entrada del grafo
        graph.set_entry_point("init")

        # Compilaci칩n del grafo para su ejecuci칩n
        self.graph = graph.compile(checkpointer=InMemorySaver())

        # Mapeo de herramientas por nombre
        self.tools = {t.name: t for t in tools}

        # Asociar las herramientas al modelo
        self.model = model.bind_tools(tools)

    def init_state(self, state: AgentState) -> AgentState:
        """Crea el estado inicial del agente con el mensaje de sistema.

        Devuelve:
            Un estado inicial con el mensaje de sistema si est치 configurado.
        """
        self._current_state = {"messages": [SystemMessage(content=self.system) if self.system else []]}
        return self._current_state
    
    def exists_action(self, state: AgentState) -> bool:
        """Indica si el 칰ltimo mensaje contiene llamadas a herramientas.

        Par치metros:
            state: estado actual con el historial de mensajes.

        Devuelve:
            ``True`` si el 칰ltimo mensaje del modelo incluye ``tool_calls``; en caso contrario ``False``.
        """
        self._current_state = state
        result = state["messages"][-1]          # 칔ltimo mensaje generado por el modelo
        return len(result.tool_calls) > 0       # Devuelve True si hay llamadas a herramientas en el 칰ltimo mensaje

    def call_openai(self, state: AgentState) -> AgentState:
        """Invoca el modelo LLM con el historial de mensajes.

        Inserta el mensaje de sistema una 칰nica vez si est치 configurado.

        Par치metros:
            state: estado actual con los mensajes acumulados.

        Devuelve:
            Un nuevo estado con el mensaje de salida del modelo en ``messages``.
        """
        self._current_state = state
        messages = state["messages"]
        message = self.model.invoke(messages)
        if self.verbose:
            message.pretty_print()
        return {"messages": [message]}

    def take_action(self, state: AgentState) -> AgentState:
        """Ejecuta las herramientas solicitadas por el modelo y retorna sus resultados.

        Par치metros:
            state: estado actual cuyo 칰ltimo mensaje contiene ``tool_calls``.

        Devuelve:
            Un estado con los mensajes de tipo ``ToolMessage`` correspondientes a cada ejecuci칩n.
        """
        self._current_state = state
        tool_calls = state["messages"][-1].tool_calls
        results = []
        for t in tool_calls:
            if self.verbose:
                print(f"\nEjecutando acci칩n: {t}\n")
            if not t["name"] in self.tools:  # check for bad tool name from LLM
                if self.verbose:
                    print("\n ....nombre de tool no v치lida....")
                result = "nombre de tool no v치lida, reintentar"  # instruir al LLM a reintentar si el nombre es incorrecto
            else:
                result = self.tools[t["name"]].invoke(t["args"])
            message = ToolMessage(tool_call_id=t["id"], name=t["name"], content=str(result))
            if self.verbose:
                message.pretty_print()
            results.append(message)
        return {"messages": results}

    def ask(self, question: str, sync: bool = False) -> AnyMessage|None:
        """Formula una pregunta al agente y devuelve la 칰ltima respuesta.

        Par치metros:
            question: texto de la consulta del usuario.

        Devuelve:
            El 칰ltimo mensaje generado por el agente en respuesta a la consulta o ``None`` si no hay respuesta o es as칤ncrono.
        """
        message = HumanMessage(content=question)
        if self.verbose:
            message.pretty_print()
        messages = [message]
        if sync:
            return self.__invoke(messages)
        else:
            return asyncio.run(self.__ainvoke(messages))

    def __invoke(self, messages: list[AnyMessage]) -> AnyMessage:
        """Formula una pregunta al agente y devuelve la 칰ltima respuesta.

        Par치metros:
            question: texto de la consulta del usuario.
        """
        return self.graph.invoke(input={"messages": messages}, config=self.config)

    async def __ainvoke(self, messages: list[AnyMessage]) -> None:
        """Formula una pregunta al agente y devuelve la 칰ltima respuesta.

        Par치metros:
            question: texto de la consulta del usuario.
        """
        print(f"\n{bold('游뱄 Wiseguy:', PURPLE)} ", end="")
        async for event in self.graph.astream_events(input={"messages": messages}, config=self.config):
            kind = event["event"]
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                # Si el contenido est치 vac칤o significa que el modelo est치 pidiendo una herramienta, por eso s칩lo imprimimos contenido no vac칤o
                if content:
                    print(content, end="")

    def print_graph(self) -> None:
        """Imprime una representaci칩n del grafo del agente."""
        print(self.graph.get_graph().draw_mermaid())